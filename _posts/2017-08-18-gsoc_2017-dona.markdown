---
title: Holmes Automated Malware Relationships
subtitle: Defining and Modeling Relationships
layout: default
modal-id: 3
category: gsoc2017
date: 2017-08-25
img: mal1.svg
thumbnail: mal1.svg
alt: license-img
repository: https://github.com/cli0/gsoc_relationship
documentation: https://github.com/cli0/gsoc_relationship/blob/primary_relationships/README.md
license: https://github.com/cli0/gsoc_relationship/blob/master/LICENSE

---

### About me

My name is Donika Mirdita. Currently, I am a full-time masters student at the Technical University of Munich, but sometimes I moonlight as a security and machine learning amateur researcher. My areas of interest are IT Security, Machine Learning and Distributed Systems. I love server farms, Big Data technology, playing with Apache projects and everything open source. I really dislike slow vpn connections and internet lag. I participated in Google Summer of Code for the Year 2017 and this blog post details the results of my work.

### Project Overview

The project topic "Holmes Automated Malware Relationships" aims to generate scored malware
relationships by using malware analytic results as generated by [Holmes-Totem](https://github.com/HolmesProcessing/Holmes-Totem) and [Holmes-Totem-Dynamic](https://github.com/HolmesProcessing/Holmes-Totem-Dynamic). This major topic was split between me and another GSoC participant.

The purpose of my project is to :

1. define the concepts and components of a malware relationship

2. model the storage for this information  

3. efficiently leverage Spark and Cassandra to run large scale malware data analytics

This project was developed as a service for Holmes Processing System and uses data and infrastructure as provided by [Holmes Processing](https://github.com/HolmesProcessing).

### Documentation

##### Defining Malware Relationships

Defining and scoring malware relationships requires a two stage process:

1. Stage 1 for extracting and pre-processing malware features of interest

2. Stage 2 to utilize the data from stage 1 for final relationships generation and scoring

During this project, I wrote the code and set up the environment for running Stage 1.
In stage 1, I look for meaningful connections between artifacts based on specific feature similarities. However, at this stage, these connections are still too raw to be considered
proper final relationships. As a result, I use the term `primary relationship` for all the object pairs that share specific similarities of interest.

##### Primary Relationships

There are 4 potential types of artifacts in our database: IP addresses, Domains, Files, and Binary Executables. All of these artifact, despite their format variety, can have
meaningful connections or `relationships` with one another.

For example: A malicious executable may issue a call to a specific domain associated with one or more IPs, which, in turn, might be related to some bot campaign. In this scenario, we can already identify several relationships:

1. Executable_1 <-> Domain_2
2. Domain_2 <-> IP_3
3. Executable_1 <-> IP_3

Each of the described connections above is a valid primary relationship.
The following picture is a high level view of the potential relationships that one can encounter in our environment.

![Relationship_Types](./img/dona/Relationship_Types.png)

##### A Few Words On The Data

The data that I use for my analytics is comprised exclusively of Totem results. Every object is uniquely identified by their sha256 hash and for each object, the system runs multiple analytic services. The system allows the existence of duplicate results for the same object and service, because for many services the results can change in time. This change can occur when analytic services are patched or expanded, antivirus signatures get updated etc.

##### Implementation: Knowledge Base Generation

The service results contain a lot of data, some of which is not necessary for the routines of this service to be carried out. As a result, I first create a Knowledge Base table which contains every feature of interest for each object and result in the database. For the generation of this table duplicate results are not ignored. The main purpose of this table is to be an authoritative repository for all pieces of relevant information. The second purpose of this intermediary table is to improve query time by forgoing the need for expensive parsing and searching for every relationship request. This table can be generated in bulk during service downtime, and as a result not burden a client wishing to calculate relationships with extra waiting time. The Knowledge Base entries have the following format:

```
analytics_knowledge_base
  - object_id text,
  - feature_type text,
  - feature_value blob,
  - timestamp timeuuid

```

The features of interest are manually defined, in this case by me through careful research. This set is by no means exhaustive and it can (and should) be extended by the analyst whenever new results types (or new research) adequately provide new relevant features. So far, my system uses the following features as indicators of relationships:

feature_type  | from service
------------- | -------------
imphash  | PEInfo
pehash | PEInfo
binary_signature | PEInfo
domain_requests | CUCKOO
yara_rules | Yara  


The Knowledge Base table stores the relevant part of the features too. Due to the potentially large size, the feature content is compressed with gzip, hence the blob type. The de/compression methods we use were slightly adapted from the following [repository](https://gist.github.com/owainlewis/1e7d1e68a6818ee4d50e). The compression method in this repository is very fast, lightweight and incurs little to no speed penalties according to my tests.

The routine for generating the Knowledge Base is a batch routine that receives a list of object IDs. This routine can be initiated whenever the user wants to populate the table with new entries. Tests on our servers have proved the routine to be fairly efficient:

```
100 hashes -> 7s
1,000 hashes -> 7s
10,000 hashes -> 23s
```

##### Implementation: Primary Relationships Generation

Primary Relationships are initial aggregations of the Knowledge Base data in order to generate and store connections between objects based on similarities of their feature values. More concretely, primary relationships store for each and every object in the database, a list of identifiers for other objects with which they share a feature similarity of sorts. For this implementation, I did not take into account duplicate results for the same object and same service. In this case, I specifically picked the newest result per object per service.

The storage for Primary Relationships was inspired in part by [TitanDB]("https://www.slideshare.net/knowfrominfo/titan-big-graph-data-with-cassandra"). For each object and each of its feature types, the primary relationships generation routine looks for matches, assigns a weight to each match, and stores the results as a Json array:

```
{
  {"sha256":<text>, "weight": <Double>},
  {"sha256":<text>, "weight": <Double>},
  ...
}
```

The weights for these primary relationships are defined depending on the feature type. For example, features like imphash, pehash and binary_signature are atomic values. For an exact same match for imphash and pehash, we assign the weight of 0.5, whereas an exact match for binary_signature we assign 1.0 . These weights are arbitrary values coming from an analyst (here: me). After extensive statistical analysis as well as [academic](https://www.usenix.org/legacy/event/leet09/tech/full_papers/wicherski/wicherski_html/)
[research](https://www.fireeye.com/blog/threat-research/2014/01/tracking-malware-import-hashing.html), I have found that objects that share these hashes have a considerable chance of being related one way or another. I chose to only look for exact matches and not cluster based on hashes. That is an approach that I tested as well, but I found that for Big Data, clustering based on the hash can create a lot of unnecessary noise for the final relationships. For features such as domain_requests and yara_rules, I use [Jaccard Similarity](https://en.wikipedia.org/wiki/Jaccard_index) to calculate the weight.

##### Storage

The following are the table schematics as generated in Cassandra.

![storage_schema](./img/dona/storage.png)

The table descriptions are as follows:

`analytics_knowledge_base` contains the Knowledge Base as described in the previous
section.

`analytics_mv_knowledge_base_by_feature` is a materialized view of `analytics_knowledge_base`
with feature_type as partition key. This table is used to enable efficient querying in
subroutines.

`analytics_primary_relationships` is the table where primary relationships are stored,
one entry per object.

### Good Times, Hard Times

The experience that I gained while doing GSoC was extremely valuable. I had the chance to discuss with experts and get valuable insights on how to approach a problem, how to model my data, and how to make compromises between efficiency and accuracy. In retrospect, this may sound pretty straightforward and easy, but when you are a student thrown into a real-world data management environment, you may end up struggling for a while until you learn to let go of what's weighing you down (your biases). GSoC was tough, but the good kind. It is something that I would warmly recommend to anyone who wants to engage in the open source community, build something beautiful, and likes to struggle for their reward.   


### Future Work

This project was a first shot at developing an operating and efficient analytics routine and there are still many more things that can be done to improve and expand it.

1. The set of feature types needs to be expanded. This can happen either by introducing new services or by applying machine learning techniques to detect classes and similarities using some non-straightforward but promising service results such as [Richheader](https://github.com/HolmesProcessing/RichHeader-Service_Collection) or [Objdump](https://github.com/HolmesProcessing/Holmes-Totem/tree/master/src/main/scala/org/holmesprocessing/totem/services/objdump).

2. Yara can be a very powerful source of information for malicious similarity and depending on the level of scrutiny, it can even go so far as providing the degree of that similarity. Weighing two yara results using Jaccard Similarity is the bare minimum one can do to calculate similarity. Not all rules are made equal. Depending on the inner conditions, some rules are more relevant than others. It may be of interest to look deeper into yara, the content of their rulesets and create a service that provides a more detailed yara similarity output. Depending on the accuracy of results, such a system could even be a standalone final relationships generator for binaries.

3. My application needs to be integrated with a pipeline that automates the process for Knowledge Base and Primary Relationships generation processes. As it stands, [Holmes-Analytics](https://github.com/HolmesProcessing/Holmes-Analytics) is where my code needs to integrate.

### Conclusions

I managed to fulfill all of my main goals so I am happy with the results of my project. There is still room for expansion and further improvements, some of which I detailed in the previous section. However, barring future extensions, I think the current iteration of the project is very sound in terms of logic and the performance is probably the best that we can get with the current setup.

### Acknowledgements

GSoC was a great experience and this was in no small part due to my mentors. Thank you for your support and your input: George Webster, Huang Xiao, Ryan Harris and Zachary Hanif. I had a great time doing this project!
