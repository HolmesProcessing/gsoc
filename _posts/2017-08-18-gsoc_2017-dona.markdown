---
title: Holmes Automated Malware Relationships
subtitle: Defining and Modeling Relationships
layout: default
modal-id: 6
category: gsoc2017
date: 2017-08-25
img: mal1.svg
thumbnail: mal1.svg
alt: license-img
repository: https://github.com/cli0/gsoc_relationship
documentation: https://github.com/cli0/gsoc_relationship/blob/master/README.md
license: https://github.com/cli0/gsoc_relationship/blob/master/LICENSE

---

### About me

My name is Donika Mirdita. Currently, I am a full-time master's student at the Technical University of Munich, but sometimes I moonlight as an amateur security and machine learning researcher. My areas of interest are IT Security, Machine Learning, and Distributed Systems. I love server farms, Big Data technology, playing with Apache projects, and everything open source. I really dislike slow VPN connections and internet lag. I participated in Google Summer of Code for the Year 2017 and this blog post details the results of my work.

### Project Overview

The project topic "Holmes Automated Malware Relationships" aims to generate scored malware
relationships by using malware analytic results as generated by [Holmes-Totem](https://github.com/HolmesProcessing/Holmes-Totem) and [Holmes-Totem-Dynamic](https://github.com/HolmesProcessing/Holmes-Totem-Dynamic). This major topic was split between me and another GSoC participant.

Defining and scoring malware relationships in a Big Data environment requires a two-stage process:

1. Stage 1: extract and pre-process malware features of interest
2. Stage 2: utilize the data from Stage 1 for final relationships generation and scoring
{: .text-left .spaced-list}

During this project, I wrote the code and set up the environment for running Stage 1.
In stage 1, I first define the most important features for each object and available analytic service. I then store these features in a lookup table to make them easily queryable and to not restrict future analytic methods through predetermining their validity. I determine meaningful similarities by going over existing research and by analyzing the data available in our Zoo. However, at this stage of the process, these connections are still too raw to be considered proper final relationships. As a result, I use the term `primary relationship` for all the similar object pairs that my system generates.

The goals of my project can be summarized as follows:

1. Define the concepts and components of a malware relationship
2. Model the storage for this information  
3. Efficiently leverage Apache Spark, Apache Cassandra and the Spark-Cassandra-Connector to run large scale malware data analytics
{: .text-left .spaced-list}

This project was developed as a service for [Holmes Processing](https://github.com/HolmesProcessing) and it is designed to efficiently use the data and infrastructure as provided by this system. However, the ideas, concepts and models can be adapted on any system that models malware relationships.

### Documentation

##### What is Stage 1 for?

Intelligent analytic systems are now part of the Big Data ecosystem. Meaningful detection systems need to be able to handle large volumes of data quickly and efficiently to satisfy client requests. As a result, analytic processes that are initiated by the client require (ideally) pre-processed, compact and easily queried data. Our analytic content (as generated by [Totem](https://github.com/HolmesProcessing/Holmes-Totem)), while very well organized, is not good enough for handling analytics on the fly. The tables can contain millions of entries, the results themselves can be large and therefore heavy, and the table format does not support fast feature extraction queries. This is not ideal if we want fast analytics. In Stage 1, I define the most important features for each object and available analytic service, and store these features so that they are both easily queried and do not restrict the analyst on the type of services he wants to build. Stage 1 is comprised of batch processes that update the content of the analytics knowledge base periodically and asynchronously from client requests.

##### Specifications

The storage cluster has 4 nodes running Apache Cassandra 3.10. Spark computations are run on a 7-node HDP 2.6 cluster. Each node has 6 cores and 64 GB of RAM.

##### A Few Words on The Data

The data that I use for my analytics contains [Totem](https://github.com/HolmesProcessing/Holmes-Totem) and [Totem-Dynamic](https://github.com/HolmesProcessing/Holmes-Totem-Dynamic) results. Every object is uniquely identified by their sha256 hash. The system runs several analytic services on each object and the results are stored using [Holmes-Storage](https://github.com/HolmesProcessing/Holmes-Storage). The system allows the existence of duplicate results for the same object and service. I did this because for many services the results can change in time. For example, this change can occur when the service itself is patched, expanded or improved; when antivirus signatures are updated etc.

##### Primary Relationships

There are 4 potential types of artifacts in our database: IP Addresses, Domains, Files, and Binary Executables. These artifacts, despite their different formats, can have
meaningful connections or `relationships` with one another.

For example: A malicious executable may issue a call to a specific domain associated with one or more IPs. In turn, these IPs might be related to some bot campaign. Assuming these connections can be detected in your Zoo in some form, you have already identified several primary relationships:

1. Executable_1 <-> Domain_2
2. Domain_2 <-> IP_3
3. Executable_1 <-> IP_3
{: .text-left .spaced-list}

Each of the described connections above is a valid primary relationship. However, the disclaimer in the previous section remains: the primary relationships do not automatically map to final relationships. Primary relationships can also be circumstantial, and that is fine. Historical analytic records should never be deleted (unless they have a specific Time To Live (TTL) threshold) and circumstantial connections can also be used to generate historical time series.

**Notice**: our database currently contains only analytic results for executables. Since I did not have any other artifact types available (IPs, domains, files), you will not see them mentioned or analyzed as objects any further. However, I have made my schematics and implementation content-independent, so that new services and artifact types can be easily integrated.

##### Implementation: Knowledge Base Generation

The service results contain a lot of data and not all that data is necessary for meaningful analytics. In addition to that, I aim to organize the data in easily queryable and parsed pieces of information to avoid unnecessary parsing and filtering during a client request. As a result, I first create a Knowledge Base table which contains every feature of interest for each object and result in the database. This table does not ignore or filter duplicate results. The main purpose of this table is to be an authoritative and easily expandable repository for all the information relevant for analytics. The second purpose of this repository is to improve query time by forgoing the need for expensive parsing and searching for every relationship request. This table can be generated in bulk during service downtime, and as a result not burden a client wishing to calculate relationships with extra waiting time. The Knowledge Base entries have the following format:

```
analytics_knowledge_base
  - object_id text,
  - feature_type text,
  - feature_value blob,
  - timestamp timeuuid
```
{: .text-left}

The features of interest are manually defined, in this case by me through careful research. This set is by no means exhaustive and it can (and should) be extended by the analyst whenever relevant information for relationship detection is defined, either by academic research or practical explorations on your dataset. So far, my system uses the following features as indicators of relationships:

| Feature Type  | Service | Feature Description |
|------------ | ----------| --------------------|
| imphash  | PEInfo | a hash of the import table for PE files |
| pehash | PEInfo | a hash for the entire structure of PE files |
| binary_signature | PEInfo | digital signature for a PE file |
| domain_requests | CUCKOO | url requests from a binary in a CUCKOO sandbox |
| yara_rules | Yara | a set of matching yara rules |
{: class="table"}


The Knowledge Base table stores the feature type and the relevant feature content too. Due to the potentially large size, the feature content (`feature_value`) is compressed with gzip, hence the blob type. I use a slightly modified version of the following [code](https://gist.github.com/owainlewis/1e7d1e68a6818ee4d50e) for de/compressing my data. The compression method in this repository is very fast, lightweight and incurs little to no speed penalties according to my tests.

Knowledge Base generation is a batch routine that requires a list of object IDs (sha256 hashes) as parameter. This routine can be initiated whenever the user wants to populate the table with new entries. Tests on our servers have proved my routine to be fairly efficient with the following performance:

```
100 hashes -> 7s
1,000 hashes -> 7s
10,000 hashes -> 23s
```
{: .text-left}

##### Implementation: Primary Relationships Generation

Primary Relationships are initial aggregations of the Knowledge Base data to generate and store connections between objects based on the similarities of their feature values. More concretely, primary relationships store, for every object in the database, identifiers of other objects with whom they share a feature similarity. For this implementation, I did not consider duplicate results for the same object and same service. In this case, I specifically picked the newest result per object per service to generate primary relationships.

The storage for Primary Relationships was inspired in part by [TitanDB]("https://www.slideshare.net/knowfrominfo/titan-big-graph-data-with-cassandra"). For each object and each of its feature types, the primary relationships generation routine looks for matches, assigns a weight to each match, and stores the weighted matches as a Json array:

```
{
  {"sha256":<text>, "weight": <Double>},
  {"sha256":<text>, "weight": <Double>},
  ...
}
```
{: .text-left}

The weights for these primary relationships are defined depending on the feature type, existing external research and observations from the analyst. Features like `imphash`, `pehash` and `binary_signature` are atomic values. For an exact same match for imphash and pehash, I assign the weight of 0.5. These weights are arbitrary values coming from an analyst (here: me). After extensive statistical analysis of my results as well as [research](https://www.usenix.org/legacy/event/leet09/tech/full_papers/wicherski/wicherski_html/) of
[academic](https://www.fireeye.com/blog/threat-research/2014/01/tracking-malware-import-hashing.html) literature, I have found that objects that share these hashes have a considerable chance of being related one way or another. In my implementation, I chose to only look for exact matches and not cluster based on hashes. I have already experimented with clustering based on malware hashes and I found that for Big Data, this approach can generate a lot of noise for the final relationships.

Unlike the hashes, the binary_signature was rarely present in the PEInfo results. These signatures are important descriptive features and yet they are rarely extractable. For example, in one of our largest results table, I could find at most a couple of hundreds of results with a binary_signature from a total of >500,000 PEInfo results. This suggests a relatively strong connection for those rare instances that have their binary_signatures *and* match exactly to another sample's signature. Because of these observations, I assign a weight of 1.0 for exact binary_signature matches. The rest of the features in my set, namely `yara_rules` and `domain_requests`, are sets of values (rules and urls respectively), and I use [Jaccard Similarity](https://en.wikipedia.org/wiki/Jaccard_index) to calculate the weight of a match.

##### Storage

The following figure shows the storage schema for the data that I generate in my implementations. You can find the code for automatically generating these tables in your Cassandra installation in the `./storage/storage.py` script.

![storage_schema](img/dona/storage.png){: .img-responsive}

|Table Name | Description |
|-----------|-------------|
|`analytics_knowledge_base`| stores the knowledge base content as described in the `Implementation` section.|
| `analytics_mv_knowledge_base_by_feature` | is a [Materialized View](https://www.datastax.com/dev/blog/new-in-cassandra-3-0-materialized-views) (MV) of the previous table. This MV is necessary to enable and speed up queries for primary relationships generation.|
| `analytics_primary_relationships` | this table stores the primary relationships as described in the `Implementation` section. This table can be [expanded](https://github.com/cli0/gsoc_relationship/blob/master/storage/README.md) for newly introduced features without having to recompute the existing content. |
{: class="table"}

### Good Times, Hard Times

The experience that I gained while doing GSoC was extremely valuable. I had the chance to discuss with experts and get valuable insights on how to efficiently approach a problem, how to model my data, and how to make compromises between efficiency and accuracy. In retrospect, this may sound straightforward and easy, but when you are a student thrown into a real-world data management environment, you may end up struggling for a while until you learn to let go of what's weighing you down (your biases). Performance improvement was also something that I had to struggle with during the project. Creating queries is easy, but one of the main goals of the project was to efficiently leverage the power of my cluster and database. I spent a lot of time improving my data modeling to make the queries as cheap as possible. My efforts were rewarded with a fast and scalable query performance.

GSoC was tough, but the good kind of tough. It is something that I would warmly recommend to anyone who wants to engage in the open source community, build something beautiful, and who likes to struggle for their reward.   

### Future Work

This project was a first shot at developing an operating and efficient analytics service and there are still many things that can be done to improve and expand it.

1. The set of feature types needs to be expanded. This can happen either by introducing new services or by applying machine learning techniques to detect classes and similarities using some non-straightforward but promising service results such as Richheader[[1](https://github.com/HolmesProcessing/RichHeader-Service_Collection), [2](https://www.sec.in.tum.de/assets/Uploads/RichHeader.pdf)] and Objdump [[3](https://github.com/HolmesProcessing/Holmes-Totem/tree/master/src/main/scala/org/holmesprocessing/totem/services/objdump), [4](https://www.sec.in.tum.de/assets/Uploads/ConvolutionalNetworks.pdf)].

2. Yara can be a powerful tool for detecting objects that share important common features. However, not all yara rules are equally relevant. I believe a similarity matching service that takes into account the content of the rules would be a meaningful tool for detecting and measuring similarities between binaries. The similarity matches produced by this service would make a neat addition to the existing implementation.

3. My application needs to be integrated with a pipeline that automates the process for Knowledge Base and Primary Relationships generation. As it stands, [Holmes-Analytics](https://github.com/HolmesProcessing/Holmes-Analytics) is where my code needs to integrate after GSoC in order to be fully integrated in the Holmes ecosystem.
{: .text-left}

### Conclusions

I managed to fulfill all my main goals so I am happy with the results of the project. There is still room for expansion and further improvements, some of which I detailed in the previous section. However, barring future extensions, I think the current iteration of the project is very sound in terms of logic and the performance is probably the best that we can get with the current setup.

### Acknowledgements

GSoC was a great experience and this was in no small part due to my mentors. Thank you for your support and your input: George Webster, Huang Xiao, Ryan Harris, and Zachary Hanif. I had a great time doing this project.
