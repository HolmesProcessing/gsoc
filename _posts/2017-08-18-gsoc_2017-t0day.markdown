---
title: Holmes Automated Malware Relationships
subtitle: Neural Network and Visualization
layout: default
modal-id: 7
category: gsoc2017
date: 2017-08-26
img: t0day_logo.svg
thumbnail: t0day_logo.svg
alt: license-img
repository: https://github.com/hi-T0day/gsoc_relationship/
documentation: https://github.com/hi-T0day/gsoc_relationship/blob/master/README.md
license: https://github.com/hi-T0day/gsoc_relationship/blob/master/LICENSE

---
### About Me 
Hello, everyone! Welcome to this blog. I am T0day. 
My nickname "T0day" is appropriate for me with many reasons: 

1. We all know what 0day is. When people said "hi-t0day", it seems like "hit 0day". That is funny.
2. The pronunciation of the Chinese word （“today” -> "今天") is similar to my real name.
3. "T" is also a significant letter in my name. 
{: .text-left .spaced-list}

Now, I am a master student at the School of Cyber Security, ShangHai JiaoTong University. My research interest is the Advanced Persistent Threat.
This blog summarizes the results of my project in the GSoC2017.  

### Project Overview
The "Holmes Automated Malware Relationships" project is divided into two parts. Dona([her blog](https://www.holmesprocessing.com/gsoc/#portfolioModal6)) and I cooperate to complete the project this summer. 
The main purpose of this project is to give the final score between the queried and the related malware by calculating the malware analytic results, and display the results in a visualization web page to help the analysts do deep-dive investigations.
Dona generates the primary relationship table and stores them in Cassandra. And my work is to:

1. In Spark, preprocess the malware analytic results generated by Holmes-Totem. 
2. In TensorFlowOnSpark, use the Neural Network to predict the label of the malware.
3. In Spark, generate the final score between the queried and the related malware.
4. In D3.js, visualize the results.
{: .text-left .spaced-list}

This project was developed as a service for Holmes Processing System and uses data and infrastructure as provided by [Holmes Processing](https://github.com/HolmesProcessing).

### Documentation

#### Preprocess the malware analytic results
The Holmes Processing provided the data analysed by Holmes-Totem and Dynamic-Holmes-Totem. My work is to preprocess the data to get the normalized data that can be used for machine learning.  
My mentor Bojan has published several papers about malware detection and analysis by machine learning. My work is based on one of his papers: 

[Kolosnjaji, Bojan, Ghadir Eraisha, George Webster, Apostolis Zarras, and Claudia Eckert. Empowering Convolutional Networks for Malware Classification and Analysis. In 30th International Joint Conference on Neural Networks (IJCNN), May 2017.](https://www.sec.in.tum.de/bojan-kolosnjaji-2/publication/452)

I refer to Bojan's code and implement the preprocessing code based on Holmes Processing database in spark, and the neural network code in TensorFlowOnSpark. 

##### Virustotal Results
Virustotal gives the malware detection signatures of different vendors. Although vendors have different naming conventions to the malware families, the keywords in signatures are almost the same.  
The preprocessing is carried out according to these steps:

1. split the words, discard the words that appear in the common prefix and suffix file, and also drop the words that size is less than 4. 
2. count the number of words in the first step, and filter out the words that appear greater than 50. In all, there are 1105 words.
3. loop through every malware and if there are keywords in his signatures, we set 1 to the corresponding keyword position, otherwise 0. The output is a 1105 dimension boolean vector.
{: .text-left .spaced-list}

##### KMeans clustering of Virustotal
KMeans algorithm is used to train the virustotal results. The 3010 samples are clustered into ten categories. The number of each category is:  
0 -> 459, 1 -> 135, 2 -> 710, 3 -> 148, 4 -> 544, 5 -> 621, 6 -> 212, 7 -> 147, 8 -> 311, 9 -> 295.

##### PEINFO
I extract int values and boolean values from the peinfo analysis results.  
The int values are  "entropy", "virtual address", "virtual size", and "size"  in the section:  ".text", ".data", ".rsrc", and ".rdata". The last item is the "timestamp". The total number of int values is 17.
In addition, I use MinMaxScaler function to normalize the int values.  
The boolean value is about dll function.
The preprocessing of boolean value is carried out according to these steps:

1. count the total number of each dll function, and filter out the dll functions which number is greater than 10000. In all, there are 502 dll functions in the list.
2. loop through every malware and if it has one dll function, we set 1 to the corresponding position, otherwise 0. The output is a 502 dimension boolean vector.  
{: .text-left .spaced-list}

Finally, I merge two vectors, and generate a 519 dimensions vector. 

##### OBJDUMP
I extract the top 10000 opcodes from the objdump services. If there is not enough opcode, then the remaining position is 0. The file "x86Opcodes" is used to convert the opcode into the number, the file gives 119 kinds of common opcodes, the opcodes that do not appear are also set to a type, then a total of 120 types of number.

#### Neural Network

##### Algorithm 

![Relationship_Types](img/t0day/NN_framework.png){: .img-responsive}

*Figure 1: the Framework of the Neural Network*

Figure 1 is the framework of the neural network. 
The peinfo data and the objdump are trained separately at first.  
The peinfo data are trained by multi-layer perceptron. There are two hidden layers, one input layer and one output layer. The number of nodes in all the layers is the same.  
The input data of objdump is the top 200 opcodes and one hot encoding is used to encode the input. It is processed by two layers of convolution, two layers of maxpooling and the output is a 720 dimension vector.  
I merge the output of peinfo and objdump, and use the softmax layer to get the prediction label.  
To the code, there are three models in the neural network. 

1. The "train" mode is used to train a model for the neural network. 
2. The "inference" mode is used to show the accuracy of the model by labeled data. 
3. The "predict" mode is used to predict the data without label
{: .text-left .spaced-list}

The neural network is run on TensorFlowOnSpark. The code works well in my own cluster with Standalone model on Spark, while it cannot run well in the HDP YARN I was using because of the wrong configuration which I could not solve. Fortunately, I pulled an issue in the Github of TensorFlowOnSpark project, and the Yahoo people responded me patiently. Although the issue has not been resolved, I believe that I can solve it with their help. [Issue here](https://github.com/yahoo/TensorFlowOnSpark/issues/119)

##### Effect
I use the neural network to predict the label of malware instead of clustering all malware with these reasons:

1. Virustotal can only be done for a minority of samples, so we do not have enough samples. The total number of the sample with VT results in this project is only 3010.
2. Virustotal results for many samples are too generic to give any useful information.
3. Even a small percent is still a lot of malware, the neural network model can label other malware with high confidence level.
{: .text-left .spaced-list}

With the labeled samples, I can tune the weight of the indicators in the primary relationship table to get the final relationship score.

#### Final Score Generate

##### Indicators of Relationship (Dona's work) 
（TIP: It is the work by Dona, another GSoC participant in this project. I reference her work here to let my blog be complete.） 

Dona manually defined the features of interest by her careful research. She used the following features as indicators of relationships and saved the results in the primary relationship table.

| Feature Type  | Service | Feature Description |
|------------ | ----------| --------------------|
| imphash  | PEInfo | a hash of the import table for PE files |
| pehash | PEInfo | a hash for the entire structure of PE files |
| binary_signature | PEInfo | digital signature for a PE file |
| domain_requests | CUCKOO | url requests from a binary in a CUCKOO sandbox |
| yara_rules | Yara | a set of matching yara rules |
{: class="table"}

##### Relationship Score Algorithm

![Relationship_Score](img/t0day/relationship_score.png){: .img-responsive}

*Figure 2: The Schematic diagram of relationship score algorithm*

The relationship score algorithm gives the similarity score between the sample queried and the related sample. Figure 2 shows the design of this algorithm. The input: rel_type scores are extracted from the primary relationship table. An algorithm that tunes the weights of each score will be shown in the next paragraph. The final relationship score is the sum of (weight × rel_type score).  
This algorithm is also used to tune the weights of each score. The neural network gives the label of the samples. The loss function is the sum of these final scores when the queried samples are in the different classifications and their final score above a threshold. Finally, minimizing the loss measured by gradient descent to get proper weights.  

This algorithm is designed by me but has not yet been implemented because I got the primary relationship table two weeks ago and time is limited. I will finish it in next term.

##### Random Final Score Generate
The final score cannot be generated from the primary table temporarily. As an alternative, I generate the JSON files by a script randomly. The script can only be run in [JSON GENERATOR](http://beta.json-generator.com) which is an online parser that can generate JSON data in a variety of formats.  
The JSON file is formatted like this:

```
To a leaf node (malware)
name:		// sha256 name
final_score:	// Final_score
indicators:	// the reason that two samples are related

To a branch node
name:		// branch_name
final_score:	// the max score of the children
indicators:	// the number of children
children: []	// the children of this node
```
{: .text-left}

#### Visualization
Visualization part is a website displaying the final relationships using d3.js. There are multiple views and tricks to improve the user experience. The page shows the relationships in a tree map.  
[See here](http://120.77.40.25/newd3/index.html)

My work is based on Rob Schmuecker's work: [D3.js Drag and Drop, Zoomable, Panning, Collapsible Tree with auto-sizing.](http://bl.ocks.org/robschmuecker/7880033) I use tree map, collapsible and zoomable figure in my web page.

##### Multiple Views
These three views can be used together.

1. Threshold View:
When we set a threshold score, those nodes whose scores are greater than the threshold are displayed.
2. Artefact View:
When we click an indicator’s name, the nodes with this feature are added to ( or deleted from ) the map.
3. Quick View:
This view shows a histogram that displays the number of nodes in different segments of scores.
{: .text-left .spaced-list}

##### Tricks

1. Collapsible:
When we click on the branch node, the branch node’s leaf nodes are folded and also the branch node is filled with steelblue.
2. Zoomable:
We can change the map scale by mouse wheel and also translate the map by dragging.
3. Tooltip:
When we move the mouse to the leaf node, the tooltip shows the final score, the reason that two samples are related, and the degree of this relation.  
When we move the mouse to the branch node, the tooltip shows the branch name, the number of children in this branch, and the maximum score in this branch.
4. Visualization weight:
In the whole map, the line weight is set according to the target node score. In each set, the nodes are sorted by the score.
{: .text-left .spaced-list}

### Proud of things
Before the project, although I know theoretical knowledge about the big data ecosystem, I lack the training of the practical ability of programming. So along with the way, I encountered a lot of problems. Through communication with mentors, reading books, most of the problems have been solved.  
These are things that make me impressed:

1. There will always be some strange analysis results generated by Holmes-Totem and Holmes-Totem-Dynamic. For example, the preprocessing code worked well when I used dataframe to show the results. However, a few days later, it failed when I save all the results into the file. I lacked the experience of large-scale data analysis and wanted to skip the wrong samples. But I did not give up, finally finally and finally, through the `try ... except` function, I find the sample, and modify the code successfully. In fact, only six wrong samples in the millions of results.
2. The TensorFlowOnSpark troubled me for a long time. In order to solve the problem: "The TensorFlowOnSpark cannot run well in the HDP YARN I was using", I pulled an issue to the author of TensorFlowOnSpark, followed George's suggestion to write the most basic “helloworld” program and use a script to run the program, talked with Bojan about the source code, did some other tests and even made the cluster on my own PC. Although the problem has not resolved, I enjoy the process of solving the problem.
3. When optimizing the visualization web page, some functions seem difficult to achieve. Things get easier when I split the problems into pieces, and then resolved. Very proud of the current interface.
{: .text-left .spaced-list}

### Future Work
Now, the third part of my work: generating the final score has not been fully achieved because the final score is generated from the primary relationship table. I got this table two weeks ago and time is not enough for me to finish it.  
I have to say, the TensorFlowOnSpark cannot work in the HDP YARN I was using. Although many methods have been used in the cluster, it still cannot work.
I will continue to follow up.  
In the next term, I will continue doing the third part work. I will spend 2-3 hours a day to implement these:

1. understanding the architecture of generating primary relationship table.
2. coding the final score generating algorithm.
3. generating the final score JSON file when querying.
{: .text-left .spaced-list}

From a longer-term perspective, the system will be online, how to make the program efficient and stable is another part of my future work. 

### Conclusions
I really enjoy this summer. Learning, coding, and talking are a wonderful time. I enjoy the wonderful open source projects: Cassandra, Spark, Zeppelin, TensorflowOnSpark, HDFS, and even Jekyll that I am writing the blog with. I learn a lot of new knowledge that leads me to the open source world. 
Considering that English is my second language, the whole summer is in English, even that we have a video conference, which imporves my English ability significantly.  
The mentors teach me a lot. I learned how to pull request better, how to finish the aims step by step, and endless knowledge.

### Acknowledgements
Thankfully Google Summer of Coding and the Honeynet Project to provide such a platform so that I can exercise and improve myself in this summer. The mentors in the Holmes Processing team are experienced and responsible, they help me solve the problem and arrange the progress, especially an e-mail from Bojan. I am sincerely grateful to my mentors: George, Bojan, Ryan and Huang. I am also particularly grateful to my partner Dona who is always patient for me to answer questions, and Manohar also helps me a lot.
